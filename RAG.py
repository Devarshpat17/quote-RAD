# -*- coding: utf-8 -*-
"""RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x6TXDzYsN3sXDtqAZ7wv91q7D1MHG42d
"""

!pip install sentence-transformers faiss-cpu transformers

from google.colab import files
import json

uploaded = files.upload()
filename = list(uploaded.keys())[0]

with open(filename, 'r') as f:
    data = json.load(f)


texts = [doc["Quote"] for doc in data]
print(f"Loaded {len(texts)} documents.")

from sentence_transformers import SentenceTransformer

embedder = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = embedder.encode(texts, show_progress_bar=True)

import faiss
import numpy as np

dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(np.array(embeddings))

def retrieve_quotes(query, k=3):
    query_vec = embedder.encode([query])
    D, I = index.search(np.array(query_vec), k)
    return [texts[i] for i in I[0]]

import os

memory_file = "memory.json"

if os.path.exists(memory_file):
    with open(memory_file, "r") as f:
        session_memory = json.load(f)
else:
    session_memory = []

MAX_MEMORY = 3

from transformers import pipeline, set_seed

generator = pipeline("text-generation", model="gpt2")
set_seed(42)

def generate_with_memory(query):
    # Retrieve from vector DB
    context = "\n".join(retrieve_quotes(query))

    # Format memory into prompt
    memory_block = ""
    if session_memory:
        recent_memory = session_memory[-MAX_MEMORY:]
        memory_block = "\n".join(
            [f"Previous Q: {pair['question']}\nPrevious A: {pair['answer']}" for pair in recent_memory]
        )

    # Construct full prompt
    prompt = f"{memory_block}\nContext:\n{context}\n\nCurrent Q: {query}\nA:"

    # Generate answer
    output = generator(prompt, max_new_tokens=100, temperature=0.7)[0]["generated_text"]

    # Store in memory
    session_memory.append({"question": query, "answer": output})

    # Save to persistent file
    with open(memory_file, "w") as f:
        json.dump(session_memory, f, indent=2)

    return output

print(generate_with_memory("Give me a quote about smiling."))
print(generate_with_memory("What do quotes say about sadness?"))

# checking memory
with open("memory.json", "r") as f:
    memory = json.load(f)
    for item in memory:
        print(item)

#dump memory
with open("memory.json", "w") as f:
    json.dump([], f)